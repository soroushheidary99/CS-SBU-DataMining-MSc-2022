{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; pd.set_option('display.max_columns', 500)\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Forward selection Implementation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUCCalculate(df, col, y):\n",
    "    X = df[col]\n",
    "    clf = RandomForestClassifier(max_depth=6, random_state=0)\n",
    "    clf.fit(X, y)\n",
    "    pred = clf.predict(X)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, pred, pos_label=2)\n",
    "    auc_score = metrics.auc(fpr, tpr)\n",
    "    return auc_score\n",
    "\n",
    "\n",
    "\n",
    "def ForwardSelection(df, target, N_Featues): \n",
    "    selected_features = ['ram']\n",
    "\n",
    "    for i in range(N_Featues-1): \n",
    "        best_res = 0 \n",
    "        for col in df_tmp.loc[:, ~df_tmp.columns.isin(selected_features)]: \n",
    "            tmp_cols = selected_features + [col]\n",
    "            auc = AUCCalculate(df, tmp_cols, target)\n",
    "            if auc > best_res: \n",
    "                best_res = auc \n",
    "                best_col = col \n",
    "        selected_features.append(best_col)\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ram', 'px_height', 'blue', 'dual_sim', 'touch_screen']\n"
     ]
    }
   ],
   "source": [
    "target = df.price_range\n",
    "df_tmp = df.drop('price_range', axis=1)\n",
    "\n",
    "features = ForwardSelection(df_tmp, target, 5)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - logreg on Forward Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cheap       0.86      0.83      0.85       500\n",
      "    Moderate       0.63      0.65      0.64       500\n",
      "  Not Cheap!       0.60      0.52      0.56       500\n",
      "   Expensive       0.78      0.88      0.83       500\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.72      0.72      0.72      2000\n",
      "weighted avg       0.72      0.72      0.72      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soroush\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df[features]\n",
    "y = df['price_range']\n",
    "price_classes = ['Cheap', 'Moderate', 'Not Cheap!', 'Expensive']\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "pred = clf.predict(X)\n",
    "\n",
    "print(classification_report(y, pred, target_names=price_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - PCA on 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlibStyle import *; setPlotly() # Custom File Delete if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 1.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(dpi=120)\n",
    "plt.title('Explain Variation Ratio Of Each Component (cum summed)')\n",
    "\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum(), drawstyle=\"steps\")\n",
    "x = np.linspace(0, 4, 5)\n",
    "plt.fill_between(x, pca.explained_variance_ratio_.cumsum(), step=\"pre\", alpha=0.4)\n",
    "plt.ylim((0.6, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>pc3</th>\n",
       "      <th>pc4</th>\n",
       "      <th>pc5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430.597094</td>\n",
       "      <td>-795.788231</td>\n",
       "      <td>-390.070331</td>\n",
       "      <td>55.636140</td>\n",
       "      <td>-48.449289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>504.984735</td>\n",
       "      <td>696.622368</td>\n",
       "      <td>-235.629081</td>\n",
       "      <td>343.925977</td>\n",
       "      <td>4.707406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>473.329828</td>\n",
       "      <td>763.942136</td>\n",
       "      <td>-680.059466</td>\n",
       "      <td>-113.916880</td>\n",
       "      <td>-4.650897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>639.822324</td>\n",
       "      <td>779.691180</td>\n",
       "      <td>-630.783647</td>\n",
       "      <td>-30.402246</td>\n",
       "      <td>8.627542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-718.985184</td>\n",
       "      <td>382.304525</td>\n",
       "      <td>591.040362</td>\n",
       "      <td>-392.357235</td>\n",
       "      <td>-0.346183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-1461.096167</td>\n",
       "      <td>843.813138</td>\n",
       "      <td>-456.014439</td>\n",
       "      <td>62.281534</td>\n",
       "      <td>33.612901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-94.445767</td>\n",
       "      <td>693.937805</td>\n",
       "      <td>708.385209</td>\n",
       "      <td>354.827455</td>\n",
       "      <td>-46.406446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>930.669266</td>\n",
       "      <td>436.671452</td>\n",
       "      <td>664.296211</td>\n",
       "      <td>136.527432</td>\n",
       "      <td>32.374260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-1252.737615</td>\n",
       "      <td>-629.884112</td>\n",
       "      <td>285.786392</td>\n",
       "      <td>-190.422713</td>\n",
       "      <td>-4.217939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1796.081521</td>\n",
       "      <td>-455.987435</td>\n",
       "      <td>-714.506259</td>\n",
       "      <td>-285.442417</td>\n",
       "      <td>-27.714154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              pc1         pc2         pc3         pc4        pc5\n",
       "0      430.597094 -795.788231 -390.070331   55.636140 -48.449289\n",
       "1      504.984735  696.622368 -235.629081  343.925977   4.707406\n",
       "2      473.329828  763.942136 -680.059466 -113.916880  -4.650897\n",
       "3      639.822324  779.691180 -630.783647  -30.402246   8.627542\n",
       "4     -718.985184  382.304525  591.040362 -392.357235  -0.346183\n",
       "...           ...         ...         ...         ...        ...\n",
       "1995 -1461.096167  843.813138 -456.014439   62.281534  33.612901\n",
       "1996   -94.445767  693.937805  708.385209  354.827455 -46.406446\n",
       "1997   930.669266  436.671452  664.296211  136.527432  32.374260\n",
       "1998 -1252.737615 -629.884112  285.786392 -190.422713  -4.217939\n",
       "1999  1796.081521 -455.987435 -714.506259 -285.442417 -27.714154\n",
       "\n",
       "[2000 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df.drop('price_range', axis=1)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "df_pca = pca.fit_transform(X)\n",
    "pd.DataFrame(df_pca, columns=[f'pc{i+1}' for i in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Logreg on pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cheap       0.54      1.00      0.70       500\n",
      "    Moderate       0.88      0.14      0.24       500\n",
      "  Not Cheap!       0.99      0.24      0.38       500\n",
      "   Expensive       0.57      1.00      0.73       500\n",
      "\n",
      "    accuracy                           0.59      2000\n",
      "   macro avg       0.75      0.59      0.51      2000\n",
      "weighted avg       0.75      0.59      0.51      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soroush\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X = df_pca\n",
    "y = df['price_range']\n",
    "price_classes = ['Cheap', 'Moderate', 'Not Cheap!', 'Expensive']\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "pred = clf.predict(X)\n",
    "\n",
    "print(classification_report(y, pred, target_names=price_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5-6 SVM Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cheap       0.98      0.99      0.98       500\n",
      "    Moderate       0.95      0.97      0.96       500\n",
      "  Not Cheap!       0.96      0.94      0.95       500\n",
      "   Expensive       0.97      0.97      0.97       500\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.97      0.97      0.97      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X, y)\n",
    "\n",
    "pred = clf.predict(X)\n",
    "print(classification_report(y, pred, target_names=price_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 - svm kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m): \n\u001b[1;32m----> 7\u001b[0m     clf \u001b[38;5;241m=\u001b[39m make_pipeline(StandardScaler(), SVC(gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel\u001b[38;5;241m=\u001b[39mk, degree\u001b[38;5;241m=\u001b[39m\u001b[43md\u001b[49m))\n\u001b[0;32m      8\u001b[0m     clf\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m     10\u001b[0m     pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "for k in ('linear', 'sigmoid'): \n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel=k, degree=d))\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    pred = clf.predict(X)\n",
    "    print('-'*30 + f' {k} ' + '-'*30)\n",
    "    print(classification_report(y, pred, target_names=price_classes))\n",
    "\n",
    "for d in ([3, 5, 7]): \n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto', kernel='poly', degree=d))\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    pred = clf.predict(X)\n",
    "    print('-'*30 + f' poly of degree {d} ' + '-'*30)\n",
    "    print(classification_report(y, pred, target_names=price_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8 - soft hard margins\n",
    "\n",
    "we can set the C parameter higher numbers to achieve what we call harder margins (eventually some value like 1e10 could be the theoretical hard margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ Using C of 0.001 ------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cheap       1.00      0.71      0.83       500\n",
      "    Moderate       0.66      0.86      0.75       500\n",
      "  Not Cheap!       0.70      0.84      0.76       500\n",
      "   Expensive       0.98      0.78      0.87       500\n",
      "\n",
      "    accuracy                           0.80      2000\n",
      "   macro avg       0.84      0.80      0.80      2000\n",
      "weighted avg       0.84      0.80      0.80      2000\n",
      "\n",
      "------------------------------ Using C of 1 ------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cheap       0.98      0.99      0.98       500\n",
      "    Moderate       0.95      0.97      0.96       500\n",
      "  Not Cheap!       0.96      0.94      0.95       500\n",
      "   Expensive       0.97      0.97      0.97       500\n",
      "\n",
      "    accuracy                           0.97      2000\n",
      "   macro avg       0.97      0.97      0.97      2000\n",
      "weighted avg       0.97      0.97      0.97      2000\n",
      "\n",
      "------------------------------ Using C of 1000 ------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cheap       1.00      1.00      1.00       500\n",
      "    Moderate       1.00      1.00      1.00       500\n",
      "  Not Cheap!       1.00      1.00      1.00       500\n",
      "   Expensive       1.00      1.00      1.00       500\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       1.00      1.00      1.00      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ([0.001, 1, 1000]): \n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto',C=c))\n",
    "    clf.fit(X, y)\n",
    "    pred = clf.predict(X)\n",
    "    print('-'*30 + f' Using C of {c} ' + '-'*30)\n",
    "    print(classification_report(y, pred, target_names=price_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9 - feature eng\n",
    "\n",
    "- for the binning, the first 40 percent of the battery powers are binned as 1 the rest are equally distanced with a factor of 20 percent each "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_labels = [1, 2, 3, 4]\n",
    "df['batter_power_binned'] = pd.qcut(df['battery_power'], q=[0, .4, .6, .8, 1], labels=bin_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['area_of_screen'] = df.px_height * df.px_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOX-COX Transformation \n",
    "\n",
    "aside of log transformation which could come in handy when we are dealing with a feature which contains very large values (and sparse) so the log transformation could smooth the values out and reduce the sparcisity even, we can use a transformation called box-cox transformation, this trnasformation comes in handy when we're using linear models, it also changes the distribution of the feature, making it more Gaussian like as some models could benefit from such transformation \n",
    "\n",
    "generally we'd want to use this transformation when we're dealing with non-homoscedastic (heteroskedasticity) data. homoscedasticity means a situation in which the variance of the dependent variable is the same for all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>battery_power</th>\n",
       "      <th>blue</th>\n",
       "      <th>clock_speed</th>\n",
       "      <th>dual_sim</th>\n",
       "      <th>fc</th>\n",
       "      <th>four_g</th>\n",
       "      <th>int_memory</th>\n",
       "      <th>m_dep</th>\n",
       "      <th>mobile_wt</th>\n",
       "      <th>n_cores</th>\n",
       "      <th>pc</th>\n",
       "      <th>px_height</th>\n",
       "      <th>px_width</th>\n",
       "      <th>ram</th>\n",
       "      <th>sc_h</th>\n",
       "      <th>sc_w</th>\n",
       "      <th>talk_time</th>\n",
       "      <th>three_g</th>\n",
       "      <th>touch_screen</th>\n",
       "      <th>wifi</th>\n",
       "      <th>price_range</th>\n",
       "      <th>batter_power_binned</th>\n",
       "      <th>area_of_screen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.882075</td>\n",
       "      <td>0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.136071</td>\n",
       "      <td>-1.156054</td>\n",
       "      <td>0.444801</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.196784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.443271</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.7</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.699365</td>\n",
       "      <td>1.620585</td>\n",
       "      <td>0.513861</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.121963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.631419</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0.9</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1.272686</td>\n",
       "      <td>1.062389</td>\n",
       "      <td>0.490345</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.359041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.484144</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>131</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>1.202879</td>\n",
       "      <td>1.208206</td>\n",
       "      <td>0.628790</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.361731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.281396</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>0.6</td>\n",
       "      <td>141</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1.190854</td>\n",
       "      <td>-0.041687</td>\n",
       "      <td>-0.590521</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.873189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>-1.004605</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>106</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>1.211870</td>\n",
       "      <td>1.422017</td>\n",
       "      <td>-1.396879</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.442795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1.563359</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.2</td>\n",
       "      <td>187</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.716882</td>\n",
       "      <td>1.574226</td>\n",
       "      <td>-0.005590</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.121159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.458414</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.7</td>\n",
       "      <td>108</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.633634</td>\n",
       "      <td>0.885256</td>\n",
       "      <td>0.863809</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.834636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.651164</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>0.1</td>\n",
       "      <td>145</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.562543</td>\n",
       "      <td>-1.384769</td>\n",
       "      <td>-1.161810</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.848518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-1.786026</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0.9</td>\n",
       "      <td>168</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.166280</td>\n",
       "      <td>-1.161290</td>\n",
       "      <td>1.534399</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.486180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  \\\n",
       "0         -0.882075     0          2.2         0   1       0           7   \n",
       "1         -0.443271     1          0.5         1   0       1          53   \n",
       "2         -1.631419     1          0.5         1   2       1          41   \n",
       "3         -1.484144     1          2.5         0   0       0          10   \n",
       "4          1.281396     1          1.2         0  13       1          44   \n",
       "...             ...   ...          ...       ...  ..     ...         ...   \n",
       "1995      -1.004605     1          0.5         1   0       1           2   \n",
       "1996       1.563359     1          2.6         1   0       0          39   \n",
       "1997       1.458414     0          0.9         1   1       1          36   \n",
       "1998       0.651164     0          0.9         0   4       1          46   \n",
       "1999      -1.786026     1          2.0         1   5       1          45   \n",
       "\n",
       "      m_dep  mobile_wt  n_cores  pc  px_height  px_width       ram  sc_h  \\\n",
       "0       0.6        188        2   2  -2.136071 -1.156054  0.444801     9   \n",
       "1       0.7        136        3   6   0.699365  1.620585  0.513861    17   \n",
       "2       0.9        145        5   6   1.272686  1.062389  0.490345    11   \n",
       "3       0.8        131        6   9   1.202879  1.208206  0.628790    16   \n",
       "4       0.6        141        2  14   1.190854 -0.041687 -0.590521     8   \n",
       "...     ...        ...      ...  ..        ...       ...       ...   ...   \n",
       "1995    0.8        106        6  14   1.211870  1.422017 -1.396879    13   \n",
       "1996    0.2        187        4   3   0.716882  1.574226 -0.005590    11   \n",
       "1997    0.7        108        8   3   0.633634  0.885256  0.863809     9   \n",
       "1998    0.1        145        5   5  -0.562543 -1.384769 -1.161810    18   \n",
       "1999    0.9        168        6  16  -0.166280 -1.161290  1.534399    19   \n",
       "\n",
       "      sc_w  talk_time  three_g  touch_screen  wifi  price_range  \\\n",
       "0        7         19        0             0     1            1   \n",
       "1        3          7        1             1     0            2   \n",
       "2        2          9        1             1     0            2   \n",
       "3        8         11        1             0     0            2   \n",
       "4        2         15        1             1     0            1   \n",
       "...    ...        ...      ...           ...   ...          ...   \n",
       "1995     4         19        1             1     0            0   \n",
       "1996    10         16        1             1     1            2   \n",
       "1997     1          5        1             1     0            3   \n",
       "1998    10         19        1             1     1            0   \n",
       "1999     4          2        1             1     1            3   \n",
       "\n",
       "     batter_power_binned  area_of_screen  \n",
       "0                      1       -2.196784  \n",
       "1                      1        1.121963  \n",
       "2                      1        1.359041  \n",
       "3                      1        1.361731  \n",
       "4                      4        0.873189  \n",
       "...                  ...             ...  \n",
       "1995                   1        1.442795  \n",
       "1996                   4        1.121159  \n",
       "1997                   4        0.834636  \n",
       "1998                   3       -0.848518  \n",
       "1999                   1       -0.486180  \n",
       "\n",
       "[2000 rows x 23 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "scaler = PowerTransformer(method = 'box-cox')\n",
    "\n",
    "df_scaled = df.copy(deep=True)\n",
    "cols = ['battery_power', 'px_height', 'ram', 'px_width', 'area_of_screen']\n",
    "for col in cols: \n",
    "    posit = df[col].apply(lambda x: 0.1 if x<=0 else x)\n",
    "    df_scaled[col] = scaler.fit_transform(posit.values.reshape(2000, 1))\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Cheap       1.00      0.99      0.99       500\n",
      "    Moderate       0.97      0.99      0.98       500\n",
      "  Not Cheap!       0.97      0.98      0.98       500\n",
      "   Expensive       0.99      0.98      0.99       500\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.98      0.98      0.98      2000\n",
      "weighted avg       0.98      0.98      0.98      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df_scaled.drop('price_range', axis=1)\n",
    "y = df_scaled.price_range\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X, y)\n",
    "\n",
    "pred = clf.predict(X)\n",
    "print(classification_report(y, pred, target_names=price_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 12 Implementing Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "X = df.drop('price_range', axis=1)\n",
    "y = df.price_range\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=10)\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "pred = clf.predict(X)\n",
    "print(classification_report(y, pred, target_names=price_classes))\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.plot(clf.feature_importances_)\n",
    "plt.xticks(np.arange(len(X.columns.to_list())), X.columns.to_list())\n",
    "plt.xticks(rotation=70)\n",
    "\n",
    "# max_depthint, default=None\n",
    "# min_samples_splitint or float, default=2 The minimum number of samples required to split an internal node:\n",
    "# min_samples_leafint or float, default=1 The minimum number of samples required to be at a leaf nod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "X = df.drop('price_range', axis=1)\n",
    "y = df.price_range\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=10, min_samples_split=10, min_samples_leaf=20)\n",
    "clf = clf.fit(X, y)\n",
    "\n",
    "pred = clf.predict(X)\n",
    "print(classification_report(y, pred, target_names=price_classes))\n",
    "\n",
    "plt.figure(dpi=200)\n",
    "plt.plot(clf.feature_importances_)\n",
    "plt.xticks(np.arange(len(X.columns.to_list())), X.columns.to_list())\n",
    "plt.xticks(rotation=70)\n",
    "\n",
    "# max_depthint, default=None\n",
    "# min_samples_splitint or float, default=2 The minimum number of samples required to split an internal node:\n",
    "# min_samples_leafint or float, default=1 The minimum number of samples required to be at a leaf nod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "75e872a0ed909aa0f9a3133d5f7519c5f56766a4fcc0ca37750e6542aed44d62"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
